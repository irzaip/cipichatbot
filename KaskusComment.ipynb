{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import secrets\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic='cemen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_structure(string):\n",
    "    return [' '.join(window) for n in range(len(string)) for\n",
    "            window in windows(string.split(), n + 1, n)]\n",
    "\n",
    "def windows(iterable, length=2, overlap=0):\n",
    "    it = iter(iterable)\n",
    "    results = list(itertools.islice(it, length))\n",
    "    while len(results) == length:\n",
    "        yield results\n",
    "        results = results[length-overlap:]\n",
    "        results.extend(itertools.islice(it, length-overlap))\n",
    "\n",
    "def clean_word(string):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    words = []\n",
    "    for word in string.split():\n",
    "        word = re.sub(pattern, ' ', word)\n",
    "        words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def make_keywords(string):\n",
    "    string = clean_word(string.lower())\n",
    "    keywords = []\n",
    "    for phrase in build_structure(string):\n",
    "        if len(list(phrase.split())) >= 1 and len(list(phrase.split())) <= 2:\n",
    "            if phrase is not None and phrase.strip() != '':\n",
    "                keywords.append(phrase)\n",
    "    return secrets.choice(keywords)\n",
    "\n",
    "def kaskus_search(topic, maxword=25):\n",
    "    \n",
    "    topic = make_keywords(topic)\n",
    "    search_url = 'https://www.kaskus.co.id/search?q='\n",
    "\n",
    "    page = requests.get(search_url+topic)\n",
    "\n",
    "    sp = bs(page.text,\"html.parser\")\n",
    "\n",
    "    #GET THE THREADS LINKS FROM MAIN PAGE\n",
    "    thrds = []\n",
    "    for a in sp.findAll(\"a\",href=True):\n",
    "        if a['href'].find('/thread/') != -1:\n",
    "            thrds.append(a['href'])\n",
    "\n",
    "    rsl = []\n",
    "    for thrd in thrds:\n",
    "        page = requests.get(thrd)\n",
    "        sp = bs(page.text,\"html.parser\")\n",
    "\n",
    "    #ambil konten di class entry - tapi dari mulai index 2\n",
    "        rs = sp.findAll('div',{'class': 'entry'})\n",
    "        for i in range(2,len(rs)):\n",
    "            tx = rs[i].getText()\n",
    "            if len(nltk.word_tokenize(tx)) < maxword:\n",
    "                rsl.append(tx)\n",
    "                \n",
    "    result=secrets.choice(rsl)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bener juga gan \n",
      "bikin rusuh aja,\n",
      "apalagi bikin ngeganggu warga\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(kaskus_search(\"cemen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
